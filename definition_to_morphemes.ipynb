{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morpheme Analysis in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import gc\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import math\n",
    "import matplotlib.pyplot as plot\n",
    "from collections import defaultdict, Counter\n",
    "from morphemes import Morphemes\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU') # idk why m1 needs this (https://stackoverflow.com/q/72441453)\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import SimpleRNN, Dense, Activation, Input, LSTM, Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder for Mapping Definitions to Word\n",
    "\n",
    "This is the structure we will be following:\n",
    "\n",
    "**Encoder**:\n",
    "* Input: a definition word encoded as an integer from 0-len(vocabulary), the vocabulary will not contain any stopwords and it will be all lowercased\n",
    "\n",
    "**Decoder**:\n",
    "* Output: a sequence of morphemes (from MorphoLex) encoded as an integer from 0-len(morpheme_lexicon), the morphology will NOT include inflectional morphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/siraire/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# fetching nltk data and setting up lemmatizer\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))\n",
    "# we will also add \"'s\" to the stops as it appears quite often\n",
    "stops.add(\"'s\")\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for later, let's reserve a start & end character to our morpheme (with embedding 0, 1)\n",
    "START_MORPHEME = '^'\n",
    "END_MORPHEME = '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the morpheme data\n",
    "morpholex_words = [] \n",
    "morphemes = set()\n",
    "morphemes_in = {}\n",
    "with open('./morphemes_files/morphemes.csv') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for word, morphemes_of_word in reader:\n",
    "        morpholex_words.append(word)\n",
    "        morphemes_of_word = morphemes_of_word.split()\n",
    "        if morphemes_of_word:\n",
    "            morphemes.update(morphemes_of_word)\n",
    "            morphemes_in[word] = [START_MORPHEME] + morphemes_of_word + [END_MORPHEME]\n",
    "\n",
    "int_to_morpheme = [START_MORPHEME, END_MORPHEME] + [m for m in morphemes] # our morpheme_lexicon!\n",
    "morpheme_to_int = {m: i for i, m in enumerate(int_to_morpheme)}\n",
    "NUMBER_MORPHEMES = len(int_to_morpheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['^', 'bio', 'log', 'y', '$']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morphemes_in['biology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the definitions of a word\n",
    "\n",
    "# we can get definitions of a word like this:\n",
    "def get_definitions(word: str) -> list[list[str]]:\n",
    "    '''\n",
    "    Returns a list of definitions where each definition is tokenized into words.\n",
    "    '''\n",
    "    syns = wordnet.synsets(word)\n",
    "    return [syn.definition() for syn in syns]\n",
    "\n",
    "def convert_tag(tag: str):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # default https://stackoverflow.com/a/46564234\n",
    "    \n",
    "def simplify_definition(definition: str) -> list[str]: # will remove stop words and lemmatize the words of the definition\n",
    "    '''\n",
    "    Takes a definition and returns a lemmatized, lowercase, destopped definition.\n",
    "    '''\n",
    "    # tokenize the definition and get pos of each word\n",
    "    dws = nltk.pos_tag(nltk.word_tokenize(definition))\n",
    "    # filter out stopwords/punctuation/lower-case:\n",
    "    dws = [(w, t) for w, t in dws if w.lower() not in stops and w.lower() not in string.punctuation]\n",
    "    # lemmatize with the pos\n",
    "    dws = [lemmatizer.lemmatize(w, convert_tag(t)) for w, t in dws]\n",
    "    return dws\n",
    "\n",
    "\n",
    "morpholex_simplified_definitions = {w: [simplify_definition(d) for d in get_definitions(w)] for w in morpholex_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['body',\n",
       "  'people',\n",
       "  'settle',\n",
       "  'far',\n",
       "  'home',\n",
       "  'maintain',\n",
       "  'tie',\n",
       "  'homeland',\n",
       "  'inhabitant',\n",
       "  'remain',\n",
       "  'national',\n",
       "  'home',\n",
       "  'state',\n",
       "  'literally',\n",
       "  'home',\n",
       "  'state',\n",
       "  'system',\n",
       "  'government'],\n",
       " ['community', 'people', 'small', 'town'],\n",
       " ['conclusive', 'resolution', 'matter', 'disposition'],\n",
       " ['act', 'colonize', 'establishment', 'colony'],\n",
       " ['something', 'settle', 'resolve', 'outcome', 'decision', 'making'],\n",
       " ['area', 'group', 'family', 'live', 'together'],\n",
       " ['termination',\n",
       "  'business',\n",
       "  'operation',\n",
       "  'use',\n",
       "  'asset',\n",
       "  'discharge',\n",
       "  'liability']]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpholex_simplified_definitions['settlement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set up the encoding scheme for the input, let's select the most common words used in definitions\n",
    "VOCAB_SIZE = 20000\n",
    "common_words = Counter()\n",
    "for word_dfs in morpholex_simplified_definitions.values():\n",
    "    for df in word_dfs:\n",
    "        common_words.update(df)\n",
    "\n",
    "vocabulary = [w for w, _ in common_words.most_common()[:VOCAB_SIZE]]\n",
    "vocabulary_to_int = {w: i for i, w in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['leave', 'immediately', 'use', 'usually', 'imperative', 'form']]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpholex_simplified_definitions[\"scram\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of encoding words as numbers, we could alternatively use a word2vec or some other form of embedding, however, my earlier tests indicated this to be infeasible to be trained on most machines, leading RAM usage to spike up to the 100GB+. However, there should be no reason the RAM usage to be this high, but I just cannot figure out how to set the model up such that used up numpy arrays/tensors get freed such that the space can be used by others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding the input and output\n",
    "def embed_definition(definition: list[str]) -> list[int]:\n",
    "    return [vocabulary_to_int[dw] for dw in definition if dw in vocabulary_to_int]\n",
    "def embed_morphology(morphemes: list[str]) -> list[int]:\n",
    "    return [morpheme_to_int[m] for m in morphemes]\n",
    "def unembed_definition(embedded_def):\n",
    "    return [vocabulary[i] for i in embedded_def]\n",
    "def unembed_morphology(embedded_m):\n",
    "    return [int_to_morpheme[m] for m in embedded_m]\n",
    "\n",
    "x_data = [] # all definitions\n",
    "y_data = [] # list of morphemes\n",
    "row_labels = [] # parallel to the rows\n",
    "for word in morpholex_words:\n",
    "    if word in morpholex_simplified_definitions and morpholex_simplified_definitions[word]:\n",
    "        for definition in morpholex_simplified_definitions[word]:\n",
    "            x_data.append(embed_definition(definition))\n",
    "            y_data.append(embed_morphology(morphemes_in[word]))\n",
    "            row_labels.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['^', 'inter', 'de', 'nomin', 'ate', 'ion', 'al', '$']\n",
      "[0, 10758, 3091, 6948, 5843, 1994, 6876, 1]\n"
     ]
    }
   ],
   "source": [
    "print(morphemes_in['interdenominational'])\n",
    "print(embed_morphology(morphemes_in['interdenominational']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6297, 459, 301, 17389]\n"
     ]
    }
   ],
   "source": [
    "print(embed_definition('''this is a definition \n",
    "see how stop words are removed'''.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6297"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_to_int['definition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[10692]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(unembed_morphology(max(y_data, key=len))))\n",
    "print(len(unembed_definition(max(x_data, key=len)))) # actually the definition of \"father\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can encode our input and output into their integer representations\n",
    "MAX_MORPHEMES_IN_WORD = 12 # output sequence length\n",
    "MAX_DEFINITION_LENGTH = 50 # input sequence length\n",
    "number_samples = len(x_data) # == len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15705"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_MORPHEMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's numpy-ify our data\n",
    "def copy_2d_list_to_nparray(lst, nparr): \n",
    "    '''you must ensure that the np array is large enough to hold \n",
    "    all values in the list'''\n",
    "    for x, row in enumerate(lst):\n",
    "        for y, val in enumerate(row):\n",
    "            nparr[x,y] = val\n",
    "    return nparr\n",
    "\n",
    "\n",
    "x_data_np = copy_2d_list_to_nparray(\n",
    "    x_data, \n",
    "    np.zeros((number_samples, MAX_DEFINITION_LENGTH)))\n",
    "y_data_np = copy_2d_list_to_nparray(\n",
    "    y_data,\n",
    "    np.zeros((number_samples, MAX_MORPHEMES_IN_WORD)))\n",
    "y_target_np = np.roll(y_data_np, -1)\n",
    "y_target_np[:, -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 12)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 50, 64)       1280000     ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)       (None, 12, 64)       1005120     ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_12 (LSTM)                 [(None, 256),        328704      ['embedding_12[0][0]']           \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_13 (LSTM)                 [(None, 12, 256),    328704      ['embedding_13[0][0]',           \n",
      "                                 (None, 256),                     'lstm_12[0][1]',                \n",
      "                                 (None, 256)]                     'lstm_12[0][2]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 12, 15705)    4036185     ['lstm_13[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,978,713\n",
      "Trainable params: 6,978,713\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = VOCAB_SIZE\n",
    "output_dim = NUMBER_MORPHEMES\n",
    "input_seq_len = MAX_DEFINITION_LENGTH\n",
    "output_seq_len = MAX_MORPHEMES_IN_WORD\n",
    "\n",
    "latent_dim = 256\n",
    "embedding_dim = 64\n",
    "\n",
    "encoder_inputs = Input(shape=(input_seq_len,))\n",
    "decoder_inputs = Input(shape=(output_seq_len,))\n",
    "\n",
    "encoder_embedding = Embedding(input_dim, embedding_dim)\n",
    "decoder_embedding = Embedding(output_dim, embedding_dim)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "encoder_states = [state_h, state_c] # since the decoder gets the states from the encoder and ignores `encoder_outputs`\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding(decoder_inputs), initial_state=encoder_states) \n",
    "\n",
    "decoder_dense = Dense(output_dim, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# model set up\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4434/4434 [==============================] - 569s 128ms/step - loss: 0.8613 - accuracy: 0.8927 - val_loss: 2.4720 - val_accuracy: 0.7913\n",
      "Epoch 2/20\n",
      "4434/4434 [==============================] - 586s 132ms/step - loss: 0.8226 - accuracy: 0.8944 - val_loss: 2.6004 - val_accuracy: 0.7955\n",
      "Epoch 3/20\n",
      "4434/4434 [==============================] - 602s 136ms/step - loss: 0.8090 - accuracy: 0.8949 - val_loss: 2.7426 - val_accuracy: 0.7970\n",
      "Epoch 4/20\n",
      "4434/4434 [==============================] - 596s 134ms/step - loss: 0.8025 - accuracy: 0.8951 - val_loss: 2.8568 - val_accuracy: 0.7966\n",
      "Epoch 5/20\n",
      "4434/4434 [==============================] - 605s 136ms/step - loss: 0.7989 - accuracy: 0.8951 - val_loss: 2.9097 - val_accuracy: 0.7971\n",
      "Epoch 6/20\n",
      "4434/4434 [==============================] - 570s 128ms/step - loss: 0.7698 - accuracy: 0.8985 - val_loss: 3.1052 - val_accuracy: 0.7973\n",
      "Epoch 7/20\n",
      "4434/4434 [==============================] - 587s 132ms/step - loss: 0.7114 - accuracy: 0.9033 - val_loss: 3.3183 - val_accuracy: 0.7978\n",
      "Epoch 8/20\n",
      "4434/4434 [==============================] - 635s 143ms/step - loss: 0.6430 - accuracy: 0.9064 - val_loss: 3.4405 - val_accuracy: 0.7982\n",
      "Epoch 9/20\n",
      "4434/4434 [==============================] - 700s 158ms/step - loss: 0.5658 - accuracy: 0.9105 - val_loss: 3.7805 - val_accuracy: 0.7977\n",
      "Epoch 10/20\n",
      "4434/4434 [==============================] - 704s 159ms/step - loss: 0.4798 - accuracy: 0.9173 - val_loss: 4.0817 - val_accuracy: 0.7984\n",
      "Epoch 11/20\n",
      "4434/4434 [==============================] - 707s 159ms/step - loss: 0.3956 - accuracy: 0.9257 - val_loss: 4.4666 - val_accuracy: 0.7985\n",
      "Epoch 12/20\n",
      "4434/4434 [==============================] - 705s 159ms/step - loss: 0.3257 - accuracy: 0.9339 - val_loss: 4.5969 - val_accuracy: 0.7986\n",
      "Epoch 13/20\n",
      "4434/4434 [==============================] - 714s 161ms/step - loss: 0.2712 - accuracy: 0.9410 - val_loss: 4.8119 - val_accuracy: 0.7987\n",
      "Epoch 14/20\n",
      "4434/4434 [==============================] - 695s 157ms/step - loss: 0.2301 - accuracy: 0.9467 - val_loss: 5.0030 - val_accuracy: 0.7984\n",
      "Epoch 15/20\n",
      "4434/4434 [==============================] - 683s 154ms/step - loss: 0.1992 - accuracy: 0.9512 - val_loss: 5.1154 - val_accuracy: 0.7986\n",
      "Epoch 16/20\n",
      "4434/4434 [==============================] - 689s 155ms/step - loss: 0.1764 - accuracy: 0.9545 - val_loss: 5.3011 - val_accuracy: 0.7988\n",
      "Epoch 17/20\n",
      "4434/4434 [==============================] - 683s 154ms/step - loss: 0.1588 - accuracy: 0.9572 - val_loss: 5.3718 - val_accuracy: 0.7988\n",
      "Epoch 18/20\n",
      "4434/4434 [==============================] - 687s 155ms/step - loss: 0.1456 - accuracy: 0.9592 - val_loss: 5.4462 - val_accuracy: 0.7988\n",
      "Epoch 19/20\n",
      "4434/4434 [==============================] - 687s 155ms/step - loss: 0.1352 - accuracy: 0.9607 - val_loss: 5.6559 - val_accuracy: 0.7989\n",
      "Epoch 20/20\n",
      "4434/4434 [==============================] - 689s 155ms/step - loss: 0.1272 - accuracy: 0.9619 - val_loss: 5.7373 - val_accuracy: 0.7987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x34636db10>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([x_data_np, y_data_np], y_target_np,\n",
    "          batch_size=32,\n",
    "          epochs=20,\n",
    "          validation_split=.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DefintionsSeq2Seq-batch32/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DefintionsSeq2Seq-batch32/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f\"./models/DefintionsSeq2Seq-batch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model in\n",
    "model = keras.models.load_model('./models/DefinitionsSeq2Seq-batch32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_12')>,\n",
       " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_12')>,\n",
       " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_12')>]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[4].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting separate encoder/decoder models from loaded in model (make sure to run the model definer first)\n",
    "\n",
    "# # fetching encoder\n",
    "# encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# # fetching decoder\n",
    "# decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Define the inference encoder model, which takes in definition input and returns encoder states\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the inference decoder model, which takes in decoder input and encoder states and returns predicted morphemes\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding(decoder_inputs), initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('video', 0.7771693)]\n",
      "[('solut', 0.33024442), ('ion', 0.9999695)]\n",
      "[('esteem', 0.17441608), ('ate', 0.85529035)]\n",
      "[('surg', 0.3888536), ('ory', 0.9905764)]\n",
      "[('differ', 0.19271624), ('ant', 0.93125033), ('ate', 0.9501478)]\n",
      "[('plank', 0.4771453)]\n"
     ]
    }
   ],
   "source": [
    "def predict_word(definition: string):\n",
    "    dws = simplify_definition(definition)\n",
    "    dws = embed_definition(dws)\n",
    "    dws = copy_2d_list_to_nparray([dws], np.zeros((1, MAX_DEFINITION_LENGTH)))\n",
    "    states_value = encoder_model.predict(dws, verbose=0)\n",
    "    target_seq = np.zeros((1,1)) # 0 should be the start morpheme, but just in case:\n",
    "    target_seq[0,0] = morpheme_to_int[START_MORPHEME]\n",
    "\n",
    "    decoded_morphemes = []\n",
    "    while True:\n",
    "        # Generate output probabilities and updated decoder states for the current target sequence and encoder states\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "\n",
    "        # Sample an output token based on the probabilities\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token_prob = output_tokens[0, -1, sampled_token_index]\n",
    "        \n",
    "        # Stop if we have reached the end token or have generated the maximum number of allowed morphemes\n",
    "        if (sampled_token_index == morpheme_to_int[END_MORPHEME]) or (len(decoded_morphemes) >= MAX_MORPHEMES_IN_WORD):\n",
    "            break\n",
    "\n",
    "        # Append the decoded morpheme to the list and update the target sequence and decoder states\n",
    "        decoded_morphemes.append((int_to_morpheme[sampled_token_index], sampled_token_prob))\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_morphemes\n",
    "\n",
    "print(predict_word('a form of entertainment broadcast on television'))\n",
    "print(predict_word('instructions for solving a problem'))\n",
    "print(predict_word('life sized computer that is sentient'))\n",
    "print(predict_word('a life threatening medical procedure on the heart performed by doctors'))\n",
    "print(predict_word('relationships between countries on different planets or in space'))\n",
    "print(predict_word('this is a definition see how stop words are removed'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
