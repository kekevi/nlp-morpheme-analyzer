{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morpheme Analysis in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import gc\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import math\n",
    "import matplotlib.pyplot as plot\n",
    "from collections import defaultdict, Counter\n",
    "from morphemes import Morphemes\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU') # idk why m1 needs this (https://stackoverflow.com/q/72441453)\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import SimpleRNN, Dense, Activation, Input, LSTM, Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder for Mapping Definitions to Word\n",
    "\n",
    "This is the structure we will be following:\n",
    "\n",
    "**Encoder**:\n",
    "* Input: a definition word encoded as an integer from 0-len(vocabulary), the vocabulary will not contain any stopwords and it will be all lowercased\n",
    "\n",
    "**Decoder**:\n",
    "* Output: a sequence of morphemes (from MorphoLex) encoded as an integer from 0-len(morpheme_lexicon), the morphology will NOT include inflectional morphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/siraire/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# fetching nltk data and setting up lemmatizer\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))\n",
    "# we will also add \"'s\" to the stops as it appears quite often\n",
    "stops.add(\"'s\")\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for later, let's reserve a start & end character to our morpheme (with embedding 0, 1)\n",
    "START_CHAR = '^'\n",
    "END_CHAR = '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the morpheme data\n",
    "morpholex_words = [] \n",
    "letters = set()\n",
    "letters_in = {}\n",
    "with open('./morphemes_files/morphemes.csv') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for word, _ in reader:\n",
    "        morpholex_words.append(word)\n",
    "        letters.update(word)\n",
    "        letters_in[word] = START_CHAR + word + END_CHAR\n",
    "\n",
    "int_to_letter = [START_CHAR, END_CHAR] + [m for m in letters] # our morpheme_lexicon!\n",
    "letter_to_int = {m: i for i, m in enumerate(int_to_letter)}\n",
    "NUMBER_LETTER = len(int_to_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^biology$'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters_in['biology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the definitions of a word\n",
    "\n",
    "# we can get definitions of a word like this:\n",
    "def get_definitions(word: str) -> list[list[str]]:\n",
    "    '''\n",
    "    Returns a list of definitions where each definition is tokenized into words.\n",
    "    '''\n",
    "    syns = wordnet.synsets(word)\n",
    "    return [syn.definition() for syn in syns]\n",
    "\n",
    "def convert_tag(tag: str):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # default https://stackoverflow.com/a/46564234\n",
    "    \n",
    "def simplify_definition(definition: str) -> list[str]: # will remove stop words and lemmatize the words of the definition\n",
    "    '''\n",
    "    Takes a definition and returns a lemmatized, lowercase, destopped definition.\n",
    "    '''\n",
    "    # tokenize the definition and get pos of each word\n",
    "    dws = nltk.pos_tag(nltk.word_tokenize(definition))\n",
    "    # filter out stopwords/punctuation/lower-case:\n",
    "    dws = [(w, t) for w, t in dws if w.lower() not in stops and w.lower() not in string.punctuation]\n",
    "    # lemmatize with the pos\n",
    "    dws = [lemmatizer.lemmatize(w, convert_tag(t)) for w, t in dws]\n",
    "    return dws\n",
    "\n",
    "\n",
    "morpholex_simplified_definitions = {w: [simplify_definition(d) for d in get_definitions(w)] for w in morpholex_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['body',\n",
       "  'people',\n",
       "  'settle',\n",
       "  'far',\n",
       "  'home',\n",
       "  'maintain',\n",
       "  'tie',\n",
       "  'homeland',\n",
       "  'inhabitant',\n",
       "  'remain',\n",
       "  'national',\n",
       "  'home',\n",
       "  'state',\n",
       "  'literally',\n",
       "  'home',\n",
       "  'state',\n",
       "  'system',\n",
       "  'government'],\n",
       " ['community', 'people', 'small', 'town'],\n",
       " ['conclusive', 'resolution', 'matter', 'disposition'],\n",
       " ['act', 'colonize', 'establishment', 'colony'],\n",
       " ['something', 'settle', 'resolve', 'outcome', 'decision', 'making'],\n",
       " ['area', 'group', 'family', 'live', 'together'],\n",
       " ['termination',\n",
       "  'business',\n",
       "  'operation',\n",
       "  'use',\n",
       "  'asset',\n",
       "  'discharge',\n",
       "  'liability']]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpholex_simplified_definitions['settlement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set up the encoding scheme for the input, let's select the most common words used in definitions\n",
    "VOCAB_SIZE = 20000\n",
    "common_words = Counter()\n",
    "for word_dfs in morpholex_simplified_definitions.values():\n",
    "    for df in word_dfs:\n",
    "        common_words.update(df)\n",
    "\n",
    "vocabulary = [w for w, _ in common_words.most_common()[:VOCAB_SIZE]]\n",
    "vocabulary_to_int = {w: i for i, w in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['leave', 'immediately', 'use', 'usually', 'imperative', 'form']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpholex_simplified_definitions[\"scram\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of encoding words as numbers, we could alternatively use a word2vec or some other form of embedding, however, my earlier tests indicated this to be infeasible to be trained on most machines, leading RAM usage to spike up to the 100GB+. However, there should be no reason the RAM usage to be this high, but I just cannot figure out how to set the model up such that used up numpy arrays/tensors get freed such that the space can be used by others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding the input and output\n",
    "def embed_definition(definition: list[str]) -> list[int]:\n",
    "    return [vocabulary_to_int[dw] for dw in definition if dw in vocabulary_to_int]\n",
    "def embed_word(word: str) -> list[int]:\n",
    "    return [letter_to_int[l] for l in word]\n",
    "def unembed_definition(embedded_def):\n",
    "    return [vocabulary[i] for i in embedded_def]\n",
    "def unembed_word(embedded_w):\n",
    "    return [int_to_letter[l] for l in embedded_w]\n",
    "\n",
    "x_data = [] # all definitions\n",
    "y_data = [] # list of morphemes\n",
    "row_labels = [] # parallel to the rows\n",
    "for word in morpholex_words:\n",
    "    if word in morpholex_simplified_definitions and morpholex_simplified_definitions[word]:\n",
    "        for definition in morpholex_simplified_definitions[word]:\n",
    "            x_data.append(embed_definition(definition))\n",
    "            y_data.append(embed_word(letters_in[word]))\n",
    "            row_labels.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^interdenominational$'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters_in['interdenominational']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(unembed_word(max(y_data, key=len))))\n",
    "print(len(unembed_definition(max(x_data, key=len)))) # actually the definition of \"father\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can encode our input and output into their integer representations\n",
    "MAX_LETTERS_IN_WORD = 35 # output sequence length\n",
    "MAX_DEFINITION_LENGTH = 50 # input sequence length\n",
    "number_samples = len(x_data) # == len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_LETTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's numpy-ify our data\n",
    "def copy_2d_list_to_nparray(lst, nparr): \n",
    "    '''you must ensure that the np array is large enough to hold \n",
    "    all values in the list'''\n",
    "    for x, row in enumerate(lst):\n",
    "        for y, val in enumerate(row):\n",
    "            nparr[x,y] = val\n",
    "    return nparr\n",
    "\n",
    "\n",
    "x_data_np = copy_2d_list_to_nparray(\n",
    "    x_data, \n",
    "    np.zeros((number_samples, MAX_DEFINITION_LENGTH)))\n",
    "y_data_np = copy_2d_list_to_nparray(\n",
    "    y_data,\n",
    "    np.zeros((number_samples, MAX_LETTERS_IN_WORD)))\n",
    "y_target_np = np.roll(y_data_np, -1)\n",
    "y_target_np[:, -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 35)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 50, 64)       1280000     ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 35, 64)       2176        ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 256),        328704      ['embedding_2[0][0]']            \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, 35, 256),    328704      ['embedding_3[0][0]',            \n",
      "                                 (None, 256),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 256)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 35, 34)       8738        ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,948,322\n",
      "Trainable params: 1,948,322\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = VOCAB_SIZE\n",
    "output_dim = NUMBER_LETTER\n",
    "input_seq_len = MAX_DEFINITION_LENGTH\n",
    "output_seq_len = MAX_LETTERS_IN_WORD\n",
    "\n",
    "latent_dim = 256\n",
    "embedding_dim = 64\n",
    "\n",
    "encoder_inputs = Input(shape=(input_seq_len,))\n",
    "decoder_inputs = Input(shape=(output_seq_len,))\n",
    "\n",
    "encoder_embedding = Embedding(input_dim, embedding_dim)\n",
    "decoder_embedding = Embedding(output_dim, embedding_dim)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "encoder_states = [state_h, state_c] # since the decoder gets the states from the encoder and ignores `encoder_outputs`\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding(decoder_inputs), initial_state=encoder_states) \n",
    "\n",
    "decoder_dense = Dense(output_dim, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# model set up\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4434/4434 [==============================] - 474s 107ms/step - loss: 0.4806 - accuracy: 0.8465 - val_loss: 0.6929 - val_accuracy: 0.7950\n",
      "Epoch 2/20\n",
      "4434/4434 [==============================] - 453s 102ms/step - loss: 0.4114 - accuracy: 0.8638 - val_loss: 0.6923 - val_accuracy: 0.8004\n",
      "Epoch 3/20\n",
      "4434/4434 [==============================] - 447s 101ms/step - loss: 0.3838 - accuracy: 0.8721 - val_loss: 0.7069 - val_accuracy: 0.8009\n",
      "Epoch 4/20\n",
      "4434/4434 [==============================] - 441s 99ms/step - loss: 0.3665 - accuracy: 0.8767 - val_loss: 0.7223 - val_accuracy: 0.8003\n",
      "Epoch 5/20\n",
      "4434/4434 [==============================] - 444s 100ms/step - loss: 0.3543 - accuracy: 0.8796 - val_loss: 0.7415 - val_accuracy: 0.7987\n",
      "Epoch 6/20\n",
      "4434/4434 [==============================] - 449s 101ms/step - loss: 0.3467 - accuracy: 0.8814 - val_loss: 0.7572 - val_accuracy: 0.8002\n",
      "Epoch 7/20\n",
      "4434/4434 [==============================] - 440s 99ms/step - loss: 0.3401 - accuracy: 0.8826 - val_loss: 0.7764 - val_accuracy: 0.7978\n",
      "Epoch 8/20\n",
      "4434/4434 [==============================] - 432s 97ms/step - loss: 0.3300 - accuracy: 0.8853 - val_loss: 0.7916 - val_accuracy: 0.7978\n",
      "Epoch 9/20\n",
      "4434/4434 [==============================] - 412s 93ms/step - loss: 0.3182 - accuracy: 0.8892 - val_loss: 0.8025 - val_accuracy: 0.7987\n",
      "Epoch 10/20\n",
      "4434/4434 [==============================] - 408s 92ms/step - loss: 0.3060 - accuracy: 0.8935 - val_loss: 0.8271 - val_accuracy: 0.7970\n",
      "Epoch 11/20\n",
      "4434/4434 [==============================] - 407s 92ms/step - loss: 0.2943 - accuracy: 0.8976 - val_loss: 0.8466 - val_accuracy: 0.7965\n",
      "Epoch 12/20\n",
      "4434/4434 [==============================] - 404s 91ms/step - loss: 0.2831 - accuracy: 0.9017 - val_loss: 0.8654 - val_accuracy: 0.7956\n",
      "Epoch 13/20\n",
      "4434/4434 [==============================] - 437s 98ms/step - loss: 0.2724 - accuracy: 0.9055 - val_loss: 0.8898 - val_accuracy: 0.7960\n",
      "Epoch 14/20\n",
      "4434/4434 [==============================] - 434s 98ms/step - loss: 0.2630 - accuracy: 0.9088 - val_loss: 0.9124 - val_accuracy: 0.7949\n",
      "Epoch 15/20\n",
      "4434/4434 [==============================] - 447s 101ms/step - loss: 0.2543 - accuracy: 0.9120 - val_loss: 0.9360 - val_accuracy: 0.7945\n",
      "Epoch 16/20\n",
      "4434/4434 [==============================] - 455s 103ms/step - loss: 0.2454 - accuracy: 0.9150 - val_loss: 0.9587 - val_accuracy: 0.7942\n",
      "Epoch 17/20\n",
      "4434/4434 [==============================] - 435s 98ms/step - loss: 0.2369 - accuracy: 0.9178 - val_loss: 0.9810 - val_accuracy: 0.7937\n",
      "Epoch 18/20\n",
      "4434/4434 [==============================] - 435s 98ms/step - loss: 0.2290 - accuracy: 0.9207 - val_loss: 0.9993 - val_accuracy: 0.7931\n",
      "Epoch 19/20\n",
      "4434/4434 [==============================] - 528s 119ms/step - loss: 0.2215 - accuracy: 0.9231 - val_loss: 1.0292 - val_accuracy: 0.7919\n",
      "Epoch 20/20\n",
      "4434/4434 [==============================] - 454s 102ms/step - loss: 0.2147 - accuracy: 0.9255 - val_loss: 1.0472 - val_accuracy: 0.7925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2fa304a30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([x_data_np, y_data_np], y_target_np,\n",
    "          batch_size=32,\n",
    "          epochs=20,\n",
    "          validation_split=.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/DefintionsSeq2Seq-batch32-LETTERS/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/DefintionsSeq2Seq-batch32-LETTERS/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f\"./models/DefintionsSeq2Seq-batch32-LETTERS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model in\n",
    "model = keras.models.load_model('./models/DefinitionsSeq2Seq-batch32-LETTERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_12')>,\n",
       " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_12')>,\n",
       " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_12')>]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[4].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting separate encoder/decoder models from loaded in model (make sure to run the model definer first)\n",
    "\n",
    "# # fetching encoder\n",
    "# encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# # fetching decoder\n",
    "# decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Define the inference encoder model, which takes in definition input and returns encoder states\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the inference decoder model, which takes in decoder input and encoder states and returns predicted morphemes\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding(decoder_inputs), initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benefiting\n",
      "relays\n",
      "statements\n",
      "fancy\n",
      "canonize\n",
      "energies\n"
     ]
    }
   ],
   "source": [
    "def predict_word(definition: string):\n",
    "    dws = simplify_definition(definition)\n",
    "    dws = embed_definition(dws)\n",
    "    dws = copy_2d_list_to_nparray([dws], np.zeros((1, MAX_DEFINITION_LENGTH)))\n",
    "    states_value = encoder_model.predict(dws, verbose=0)\n",
    "    target_seq = np.zeros((1,1)) # 0 should be the start morpheme, but just in case:\n",
    "    target_seq[0,0] = letter_to_int[START_CHAR]\n",
    "\n",
    "    decoded_letters = []\n",
    "    while True:\n",
    "        # Generate output probabilities and updated decoder states for the current target sequence and encoder states\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "\n",
    "        # Sample an output token based on the probabilities\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token_prob = output_tokens[0, -1, sampled_token_index]\n",
    "        \n",
    "        # Stop if we have reached the end token or have generated the maximum number of allowed morphemes\n",
    "        if (sampled_token_index == letter_to_int[END_CHAR]) or (len(decoded_letters) >= MAX_LETTERS_IN_WORD):\n",
    "            break\n",
    "\n",
    "        # Append the decoded morpheme to the list and update the target sequence and decoder states\n",
    "        decoded_letters.append((int_to_letter[sampled_token_index], sampled_token_prob))\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ''.join(l for l, _ in decoded_letters)\n",
    "\n",
    "print(predict_word('a form of entertainment broadcast on television'))\n",
    "print(predict_word('instructions for solving a problem'))\n",
    "print(predict_word('life sized computer that is sentient'))\n",
    "print(predict_word('a life threatening medical procedure on the heart performed by doctors'))\n",
    "print(predict_word('relationships between countries on different planets or in space'))\n",
    "print(predict_word('a red or green fruit that grows on trees'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
